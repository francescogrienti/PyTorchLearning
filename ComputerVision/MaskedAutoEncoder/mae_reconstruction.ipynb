{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:55:47.227371Z",
     "start_time": "2025-04-07T08:55:47.175320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from ComputerVision.MaskedAutoEncoder.trainer.train_mae_v1 import MaskedAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5abe38b4e337bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.manual_seed(1337)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "print(f\"Using device: {device}\", flush=True)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fed10b0343233ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the utils\n",
    "cifar_mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "cifar_std = np.array([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "def show(img, ax=None, title=None):\n",
    "    \"\"\"Utility function to display an image\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    img = img.cpu().detach().numpy()\n",
    "    if img.shape[0] == 1:  # Grayscale image\n",
    "        img = img[0]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "    elif img.shape[-1] == 3:  # Already HWC\n",
    "        img = img * cifar_std + cifar_mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "    else:  # CHW\n",
    "        img = img.transpose((1, 2, 0))  # Convert to HWC\n",
    "        img = img * cifar_std + cifar_mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "def prepare_model(checkpoint):\n",
    "    mae_model = MaskedAutoEncoder(\n",
    "        embed_size=128,\n",
    "        decoder_embed_size=64, num_patches=256,\n",
    "        forward_expansion=256, dropout=0.1, num_attention_heads=8,\n",
    "        qvk_bias=True, dataset=test_dataset, patch_size=2,\n",
    "        encod_hidden_layers=8, decod_hidden_layers=4,\n",
    "        num_channels=3\n",
    "    )\n",
    "    # load model\n",
    "    checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "    mae_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    mae_model.eval()\n",
    "    return mae_model\n",
    "\n",
    "def run_one_image(x, model):\n",
    "    # make it a batch-like\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    #x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # run MAE\n",
    "    loss, y, mask = model(x.float(), mask_ratio=0.75)\n",
    "    y = model.unpatchify(y)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "\n",
    "    # visualize the mask\n",
    "    mask = mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_size**2 *3)  # (N, H*W, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "\n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show(x[0], title=\"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show(im_masked[0],title= \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show(y[0], title=\"reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show(im_paste[0],title= \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b040dea9cf99dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.1589322..1.9320949].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load an image\n",
    "img_1, _ = test_dataset[np.random.randint(len(test_dataset))]  # Get the first test image\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "show(img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40811b435ae321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoint.pth'\n",
    "model_mae = prepare_model(checkpoint)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff168bf3cafb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random mask reproducible (comment out to make it change)\n",
    "print('MAE with pixel reconstruction:')\n",
    "run_one_image(img_1, model_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92182210843eef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE with pixel reconstruction:')\n",
    "img_2, _ = test_dataset[1]  # Get the first test image\n",
    "img_1 = img_1.unsqueeze(0).to(device)  # Add batch dimension\n",
    "run_one_image(img_2, model_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa5af8c62ba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE with pixel reconstruction:')\n",
    "img_3, _ = test_dataset[2]  # Get the first test image\n",
    "img_3 = img_3.unsqueeze(0).to(device)  # Add batch dimension\n",
    "run_one_image(img_3, model_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41a91dc3bf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE with pixel reconstruction:')\n",
    "img_4, _ = test_dataset[3]  # Get the first test image\n",
    "img_4 = img_4.unsqueeze(0).to(device)  # Add batch dimension\n",
    "run_one_image(img_4, model_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
